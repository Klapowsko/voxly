from pathlib import Path
import re

from anyio import to_thread

from app.config import Settings


PROMPT_TOPICOS = """Voc√™ √© um especialista em an√°lise e organiza√ß√£o de conte√∫do. Analise o seguinte texto transcrito de um √°udio e crie uma estrutura de t√≥picos PROFUNDA, DETALHADA e COMPLETA, cobrindo TODOS os assuntos importantes mencionados.

INSTRU√á√ïES DETALHADAS:

1. COMPREENS√ÉO COMPLETA DO CONTE√öDO:
   - Leia e compreenda COMPLETAMENTE todo o texto, sem pular partes
   - Identifique TODOS os principais temas, conceitos e ideias apresentados
   - Entenda o contexto e a mensagem central
   - Reconhe√ßa refer√™ncias b√≠blicas, hist√≥ricas ou culturais mencionadas
   - Identifique TODAS as transi√ß√µes de assunto e mudan√ßas de tema
   - N√£o deixe nenhum assunto importante de fora

2. ORGANIZA√á√ÉO EM T√ìPICOS (GERE T√ìPICOS PARA TODOS OS ASSUNTOS):
   - Crie t√≥picos tem√°ticos para CADA assunto importante mencionado
   - N√ÉO limite a quantidade de t√≥picos - crie quantos forem necess√°rios para cobrir todo o conte√∫do
   - Cada t√≥pico deve ter um T√çTULO DESCRITIVO e espec√≠fico que resume o conte√∫do
   - N√ÉO agrupe assuntos diferentes em um √∫nico t√≥pico - cada assunto importante deve ter seu pr√≥prio t√≥pico
   - Agrupe apenas ideias MUITO relacionadas no mesmo t√≥pico
   - Use subt√≥picos quando necess√°rio para melhor organiza√ß√£o
   - Identifique CADA mudan√ßa de assunto como um novo t√≥pico
   - Seja generoso na cria√ß√£o de t√≥picos - √© melhor ter mais t√≥picos bem organizados do que poucos t√≥picos gen√©ricos

3. CONTE√öDO DETALHADO E COMPLETO (N√ÉO SEJA SUCINTO):
   - Cada t√≥pico deve ter CONTE√öDO SUBSTANCIAL e completo
   - Inclua TODO o contexto relevante do que foi dito sobre aquele assunto
   - Adicione explica√ß√µes e detalhes importantes
   - N√ÉO resuma demais - preserve TODAS as informa√ß√µes relevantes
   - Inclua cita√ß√µes diretas quando forem importantes
   - Desenvolva cada ideia com profundidade e completude
   - Certifique-se de que nenhuma informa√ß√£o importante seja perdida

4. COMENT√ÅRIOS E AN√ÅLISES DA IA:
   - Adicione uma se√ß√£o de "üí° An√°lise" ou "üìù Coment√°rios" no in√≠cio de cada t√≥pico
   - Forne√ßa insights, interpreta√ß√µes e observa√ß√µes sobre o conte√∫do
   - Destaque pontos-chave importantes e sua relev√¢ncia
   - Adicione conex√µes entre ideias quando apropriado
   - Forne√ßa contexto adicional quando necess√°rio
   - Se houver refer√™ncias b√≠blicas, explique seu significado e contexto
   - Adicione observa√ß√µes sobre a import√¢ncia ou aplica√ß√£o pr√°tica do conte√∫do

5. FORMATA√á√ÉO:
   - Use par√°grafos bem formatados (n√£o tudo em uma linha)
   - Quebre o texto em par√°grafos l√≥gicos de 3-5 linhas
   - Use formata√ß√£o Markdown apropriada (## para t√≠tulos, ** para √™nfase, - para listas)
   - Adicione espa√ßamento adequado entre se√ß√µes
   - Use emojis para destacar se√ß√µes importantes (üí°, üìù, ‚ö†Ô∏è, etc.)

6. ESTRUTURA ESPERADA PARA CADA T√ìPICO:
   ```
   ## [T√≠tulo Descritivo e Espec√≠fico do T√≥pico]
   
   üí° **An√°lise e Coment√°rios:**
   [Sua an√°lise completa, insights e coment√°rios sobre este t√≥pico - explique o que √© importante, por que √© relevante, e adicione contexto detalhado]
   
   **Conte√∫do:**
   [Conte√∫do completo e detalhado do t√≥pico, com m√∫ltiplos par√°grafos desenvolvendo TODAS as ideias relacionadas]
   
   [Mais par√°grafos com detalhes adicionais e informa√ß√µes complementares]
   
   [Subt√≥picos se necess√°rio para organizar melhor o conte√∫do]
   ```

7. COBERTURA COMPLETA:
   - Analise TODO o texto, do in√≠cio ao fim
   - Crie t√≥picos para TODOS os assuntos importantes mencionados
   - N√£o deixe nenhuma parte significativa do conte√∫do sem um t√≥pico correspondente
   - Adapte a quantidade de t√≥picos ao tamanho e complexidade do conte√∫do
   - Para conte√∫dos longos (1 hora ou mais), crie t√≥picos suficientes para cobrir tudo
   - Para conte√∫dos mais curtos, ainda assim crie t√≥picos detalhados para cada assunto

TEXTO A ANALISAR:
{texto}

Gere os t√≥picos organizados seguindo EXATAMENTE as instru√ß√µes acima. Seja MUITO detalhado, formatado, inteligente na organiza√ß√£o e gere t√≥picos COMPLETOS para TODOS os assuntos importantes, sem deixar nada de fora. O objetivo √© ter uma cobertura completa e detalhada de todo o conte√∫do."""


def formatar_resultado_ia(texto: str) -> str:
    """Formata o resultado da IA para garantir qualidade."""
    # Remove espa√ßos m√∫ltiplos
    texto = re.sub(r" +", " ", texto)
    # Garante quebra de linha ap√≥s t√≠tulos
    texto = re.sub(r"(## .+?)([^\n])", r"\1\n\2", texto)
    # Garante par√°grafos (quebra dupla ap√≥s par√°grafos longos)
    texto = re.sub(r"\. ([A-Z][^.!?]{50,})", r".\n\n\1", texto)
    return texto.strip()


def usar_ollama(texto: str, modelo: str = "llama3.2", ollama_url: str = "http://localhost:11434") -> str | None:
    """Usa Ollama para gerar t√≥picos (requer servidor Ollama rodando)."""
    try:
        import ollama
    except ImportError:
        return None
    
    # Usa o texto completo ou o m√°ximo que o modelo suportar
    # Para modelos grandes, podemos usar textos muito longos
    # Limite aumentado significativamente para suportar √°udios longos
    texto_limitado = texto[:100000] if len(texto) > 100000 else texto
    prompt = PROMPT_TOPICOS.format(texto=texto_limitado)
    
    try:
        # Tenta usar a biblioteca ollama
        response = ollama.chat(
            model=modelo,
            messages=[
                {
                    "role": "system",
                    "content": "Voc√™ √© um especialista em an√°lise de conte√∫do, organiza√ß√£o de informa√ß√µes e formata√ß√£o de textos. Sempre siga as instru√ß√µes detalhadamente. Gere M√öLTIPLOS t√≥picos com conte√∫do substancial e coment√°rios anal√≠ticos."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            options={
                "temperature": 0.4,  # Um pouco mais de criatividade para coment√°rios
                "num_predict": 16000,  # Permite respostas muito longas para conte√∫dos extensos
            }
        )
        resultado = response["message"]["content"]
        return formatar_resultado_ia(resultado)
    except Exception:
        # Se falhar, tenta usar API HTTP diretamente
        try:
            import requests
            response = requests.post(
                f"{ollama_url}/api/chat",
                json={
                    "model": modelo,
                    "messages": [
                        {
                            "role": "system",
                            "content": "Voc√™ √© um especialista em an√°lise de conte√∫do, organiza√ß√£o de informa√ß√µes e formata√ß√£o de textos. Sempre siga as instru√ß√µes detalhadamente."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "stream": False,
                    "options": {
                        "temperature": 0.4,
                        "num_predict": 16000,
                    }
                },
                timeout=600  # 10 minutos
            )
            if response.status_code == 200:
                resultado = response.json()["message"]["content"]
                return formatar_resultado_ia(resultado)
        except Exception:
            pass
        
        return None


def usar_huggingface(texto: str) -> str | None:
    """Usa Hugging Face Transformers para an√°lise e organiza√ß√£o."""
    try:
        from transformers import pipeline
    except ImportError:
        return None
    
    try:
        # Usa um modelo de sumariza√ß√£o para extrair pontos-chave
        summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=-1  # CPU
        )
        
        # Divide o texto em chunks menores para processar mais conte√∫do
        # Chunks menores permitem mais t√≥picos
        palavras = texto.split()
        palavras_por_chunk = 400  # Chunks menores para mais granularidade
        num_chunks = max(10, len(palavras) // palavras_por_chunk)  # M√≠nimo 10 chunks, mais se necess√°rio
        
        chunks = []
        for i in range(0, len(palavras), palavras_por_chunk):
            chunk = " ".join(palavras[i:i + palavras_por_chunk])
            if len(chunk.strip()) > 50:  # Apenas chunks significativos
                chunks.append(chunk)
        
        # Processa cada chunk e preserva conte√∫do original
        topicos_com_conteudo = []
        
        for i, chunk in enumerate(chunks):
            try:
                # Gera resumo do chunk
                resultado = summarizer(
                    chunk,
                    max_length=200,  # Resumos maiores
                    min_length=80,   # M√≠nimo maior para mais detalhes
                    do_sample=False
                )
                resumo = resultado[0]["summary_text"]
                
                # Identifica tema automaticamente do chunk
                tema = identificar_tema_automatico(chunk, resumo)
                
                # Preserva parte do conte√∫do original junto com o resumo
                topicos_com_conteudo.append({
                    "tema": tema,
                    "resumo": resumo,
                    "conteudo_original": chunk[:500],  # Primeiros 500 caracteres do conte√∫do original
                    "indice": i
                })
            except Exception:
                continue
        
        if topicos_com_conteudo:
            return formatar_topicos_huggingface_melhorado(topicos_com_conteudo)
        
        return None
    except Exception:
        return None


def identificar_tema_automatico(conteudo_original: str, resumo: str) -> str:
    """Identifica o tema principal automaticamente baseado no conte√∫do, sem usar lista fixa."""
    from collections import Counter
    
    # Combina conte√∫do e resumo para an√°lise
    texto_completo = (conteudo_original + " " + resumo).strip()
    
    # Extrai palavras-chave importantes (palavras com 4+ letras)
    palavras = re.findall(r"\b\w{4,}\b", texto_completo.lower())
    
    # Remove palavras comuns que n√£o s√£o informativas
    palavras_comuns = {
        "isso", "aqui", "onde", "quando", "como", "para", "com", "sobre",
        "mais", "muito", "pode", "ser√°", "seria", "tempo", "momento", "pessoa",
        "pessoas", "coisa", "coisas", "tipo", "tipos", "forma", "maneira",
        "tamb√©m", "ainda", "sempre", "nunca", "depois", "antes", "agora",
        "ent√£o", "assim", "dessa", "desse", "deste", "desta", "todo", "toda"
    }
    
    palavras_filtradas = [p for p in palavras if p not in palavras_comuns]
    
    # Conta frequ√™ncia das palavras
    contador = Counter(palavras_filtradas)
    
    # Pega as palavras mais frequentes e significativas
    palavras_principais = [palavra for palavra, _ in contador.most_common(8)]
    
    # Tenta criar t√≠tulo inteligente baseado no conte√∫do
    # Primeiro, tenta usar o in√≠cio do resumo se for descritivo
    if resumo:
        primeira_frase = resumo.split(".")[0].strip()
        # Se a primeira frase for razo√°vel (20-100 caracteres), usa ela
        if 20 <= len(primeira_frase) <= 100:
            # Limpa a frase removendo palavras muito comuns no in√≠cio
            palavras_frase = primeira_frase.split()
            if len(palavras_frase) > 3:
                return primeira_frase
    
    # Se n√£o, cria t√≠tulo baseado nas palavras principais
    if palavras_principais:
        # Pega as 2-3 palavras mais relevantes
        palavras_titulo = palavras_principais[:3]
        titulo = " ".join(palavras_titulo).title()
        
        # Se o t√≠tulo for muito curto, adiciona contexto
        if len(titulo) < 15:
            # Tenta pegar uma frase do conte√∫do original
            primeira_sentenca = conteudo_original.split(".")[0].strip()
            if 30 <= len(primeira_sentenca) <= 80:
                return primeira_sentenca[:60]
        
        return titulo
    
    # Fallback: usa in√≠cio do conte√∫do
    primeira_parte = conteudo_original[:70].strip()
    if primeira_parte:
        # Remove pontua√ß√£o final se houver
        primeira_parte = re.sub(r"[.!?]+$", "", primeira_parte)
        return primeira_parte
    
    return "T√≥pico do Conte√∫do"


def formatar_topicos_huggingface_melhorado(topicos_com_conteudo: list) -> str:
    """Formata os t√≥picos do Hugging Face com conte√∫do detalhado."""
    resultado = "# T√≥picos Organizados da Transcri√ß√£o\n\n"
    resultado += "*An√°lise realizada com Hugging Face Transformers*\n\n"
    
    # Agrupa t√≥picos por tema, mas mant√©m separados se forem temas diferentes
    topicos_agrupados = {}
    for topico in topicos_com_conteudo:
        tema = topico["tema"]
        if tema not in topicos_agrupados:
            topicos_agrupados[tema] = []
        topicos_agrupados[tema].append(topico)
    
    # Cria um t√≥pico para cada grupo, mas se houver muitos do mesmo tema, divide
    indice = 1
    for tema, topicos_tema in topicos_agrupados.items():
        # Se houver muitos t√≥picos do mesmo tema, divide em subt√≥picos
        if len(topicos_tema) > 2:
            # Cria um t√≥pico principal e subt√≥picos
            resultado += f"## {indice}. {tema}\n\n"
            resultado += "üí° **An√°lise:**\n"
            resultado += f"Este tema √© abordado em {len(topicos_tema)} momentos diferentes do conte√∫do, demonstrando sua import√¢ncia.\n\n"
            resultado += "**Conte√∫do:**\n\n"
            
            for sub_indice, topico in enumerate(topicos_tema, 1):
                resultado += f"### {indice}.{sub_indice} - {tema} (Parte {sub_indice})\n\n"
                resultado += f"**Resumo:** {topico['resumo']}\n\n"
                resultado += f"**Conte√∫do detalhado:**\n\n"
                # Formata o conte√∫do original em par√°grafos
                conteudo = topico["conteudo_original"]
                sentencas = re.split(r"([.!?]+)", conteudo)
                paragrafo = []
                
                for j in range(0, len(sentencas) - 1, 2):
                    if j + 1 < len(sentencas):
                        sentenca = (sentencas[j] + sentencas[j + 1]).strip()
                        if sentenca and len(sentenca) > 20:
                            paragrafo.append(sentenca)
                            if len(paragrafo) >= 2:
                                resultado += " ".join(paragrafo) + "\n\n"
                                paragrafo = []
                
                if paragrafo:
                    resultado += " ".join(paragrafo) + "\n\n"
                
                resultado += "\n"
            
            resultado += "---\n\n"
            indice += 1
        else:
            # T√≥picos √∫nicos ou poucos - cria t√≥pico individual
            for topico in topicos_tema:
                resultado += f"## {indice}. {tema}\n\n"
                resultado += "üí° **An√°lise:**\n"
                resultado += f"{topico['resumo']}\n\n"
                resultado += "**Conte√∫do detalhado:**\n\n"
                
                # Formata o conte√∫do original em par√°grafos
                conteudo = topico["conteudo_original"]
                sentencas = re.split(r"([.!?]+)", conteudo)
                paragrafo = []
                
                for j in range(0, len(sentencas) - 1, 2):
                    if j + 1 < len(sentencas):
                        sentenca = (sentencas[j] + sentencas[j + 1]).strip()
                        if sentenca and len(sentenca) > 20:
                            paragrafo.append(sentenca)
                            if len(paragrafo) >= 2:
                                resultado += " ".join(paragrafo) + "\n\n"
                                paragrafo = []
                
                if paragrafo:
                    resultado += " ".join(paragrafo) + "\n\n"
                
                resultado += "---\n\n"
                indice += 1
    
    return resultado


def gerar_topicos_simples(texto: str) -> str:
    """Gera t√≥picos melhorados usando processamento de texto inteligente (fallback final)."""
    # Remove espa√ßos m√∫ltiplos
    texto = re.sub(r"\s+", " ", texto).strip()
    
    # Calcula quantos t√≥picos criar baseado no tamanho do texto
    palavras = texto.split()
    num_palavras = len(palavras)
    # Adapta dinamicamente: aproximadamente 1 t√≥pico a cada 200-300 palavras
    # Mas permite mais t√≥picos para conte√∫dos maiores
    palavras_por_topico = max(200, min(300, num_palavras // max(6, num_palavras // 500)))
    
    # Divide o texto em partes baseado em pontua√ß√£o e tamanho
    partes = re.split(r"([.!?]+)", texto)
    
    # Reconstr√≥i senten√ßas completas
    sentencas = []
    buffer = ""
    for parte in partes:
        parte = parte.strip()
        if not parte:
            continue
        
        # Se a parte tem pontua√ß√£o ou √© muito longa, finaliza senten√ßa
        if re.search(r"[.!?]$", parte) or len(buffer) > 150:
            if buffer:
                sentenca_completa = (buffer + " " + parte).strip()
                if len(sentenca_completa) > 40:  # Apenas senten√ßas significativas
                    sentencas.append(sentenca_completa)
                buffer = ""
            else:
                if len(parte) > 40:
                    sentencas.append(parte)
        else:
            buffer = (buffer + " " + parte).strip()
    
    if buffer and len(buffer) > 40:
        sentencas.append(buffer)
    
    # Se ainda n√£o tiver senten√ßas suficientes, divide por tamanho
    if len(sentencas) < palavras_por_topico:
        palavras = texto.split()
        tamanho_chunk = palavras_por_topico
        sentencas = []
        for i in range(0, len(palavras), tamanho_chunk):
            chunk = " ".join(palavras[i:i + tamanho_chunk])
            if len(chunk) > 40:
                sentencas.append(chunk)
    
    # Divide senten√ßas em grupos para criar t√≥picos dinamicamente
    # Cria m√∫ltiplos t√≥picos baseado no conte√∫do - sem limite m√°ximo r√≠gido
    # Adapta ao tamanho: mais senten√ßas = mais t√≥picos poss√≠veis
    num_topicos = max(6, len(sentencas) // 3)  # M√≠nimo 6, mas pode ser muito mais para conte√∫dos longos
    sentencas_por_topico = len(sentencas) // num_topicos if num_topicos > 0 else len(sentencas)
    
    # Cria t√≥picos dinamicamente com identifica√ß√£o autom√°tica de temas
    from collections import Counter
    
    resultado = "# T√≥picos Organizados da Transcri√ß√£o\n\n"
    resultado += "*An√°lise e organiza√ß√£o inteligente do conte√∫do*\n\n"
    
    for i in range(num_topicos):
        inicio = i * sentencas_por_topico
        fim = inicio + sentencas_por_topico if i < num_topicos - 1 else len(sentencas)
        sentencas_topo = sentencas[inicio:fim]
        
        if not sentencas_topo:
            continue
        
        # Identifica tema automaticamente baseado no conte√∫do deste t√≥pico
        conteudo_topo = " ".join(sentencas_topo)
        
        # Extrai palavras-chave do t√≥pico
        palavras_topo = re.findall(r"\b\w{4,}\b", conteudo_topo.lower())
        palavras_comuns = {
            "isso", "aqui", "onde", "quando", "como", "para", "com", "sobre",
            "mais", "muito", "pode", "ser√°", "seria", "tempo", "momento"
        }
        palavras_filtradas = [p for p in palavras_topo if p not in palavras_comuns]
        
        # Conta frequ√™ncia
        contador = Counter(palavras_filtradas)
        palavras_principais = [palavra for palavra, _ in contador.most_common(5)]
        
        # Cria t√≠tulo baseado nas palavras principais ou primeira senten√ßa
        if palavras_principais:
            titulo = " ".join(palavras_principais[:3]).title()
            if len(titulo) < 10:
                # Se t√≠tulo muito curto, usa primeira senten√ßa
                primeira_sentenca = sentencas_topo[0].strip()
                if len(primeira_sentenca) > 20:
                    titulo = primeira_sentenca[:60].rstrip(".,!?")
        else:
            # Usa primeira senten√ßa como t√≠tulo
            primeira_sentenca = sentencas_topo[0].strip()
            titulo = primeira_sentenca[:60].rstrip(".,!?") if len(primeira_sentenca) > 20 else f"T√≥pico {i + 1}"
        
        resultado += f"## {i + 1}. {titulo}\n\n"
        resultado += "üí° **An√°lise:**\n"
        if palavras_principais:
            resultado += f"Este t√≥pico aborda aspectos relacionados a {', '.join(palavras_principais[:3])}. "
        else:
            resultado += "Este t√≥pico aborda aspectos importantes do conte√∫do apresentado. "
        resultado += "O conte√∫do desenvolve ideias importantes sobre este tema.\n\n"
        resultado += "**Conte√∫do:**\n\n"
        
        # Formata senten√ßas em par√°grafos bem estruturados
        paragrafo = []
        for sentenca in sentencas_topo:
            # Adiciona pontua√ß√£o se n√£o tiver
            if not re.search(r"[.!?]$", sentenca):
                sentenca += "."
            
            paragrafo.append(sentenca)
            
            # Cria par√°grafos de 2-3 senten√ßas
            if len(paragrafo) >= 2:
                texto_paragrafo = " ".join(paragrafo)
                resultado += texto_paragrafo + "\n\n"
                paragrafo = []
        
        # Adiciona par√°grafo restante
        if paragrafo:
            texto_paragrafo = " ".join(paragrafo)
            if not re.search(r"[.!?]$", texto_paragrafo):
                texto_paragrafo += "."
            resultado += texto_paragrafo + "\n\n"
        
        resultado += "---\n\n"
    
    return resultado


async def generate_topics_markdown(
    transcript: str, settings: Settings, request_id: str
) -> tuple[str, Path]:
    """Gera conte√∫do em Markdown usando modelos open source (Ollama/Hugging Face/m√©todo simples)."""
    output_path = settings.outputs_dir / f"{request_id}_topics.md"
    
    def _run() -> tuple[str, Path]:
        resultado = None
        
        # Tenta Ollama primeiro
        if settings.ollama_model:
            resultado = usar_ollama(transcript, settings.ollama_model, settings.ollama_url)
            if resultado:
                output_path.write_text(resultado, encoding="utf-8")
                return resultado, output_path
        
        # Se Ollama n√£o funcionou, tenta Hugging Face
        if not resultado:
            resultado = usar_huggingface(transcript)
            if resultado:
                output_path.write_text(resultado, encoding="utf-8")
                return resultado, output_path
        
        # Fallback para m√©todo simples
        if not resultado or len(resultado.strip()) < 100:
            resultado = gerar_topicos_simples(transcript)
        
        # Garante que o resultado n√£o est√° vazio
        if not resultado or len(resultado.strip()) < 50:
            resultado = gerar_topicos_simples(transcript)
        
        output_path.write_text(resultado, encoding="utf-8")
        return resultado, output_path
    
    return await to_thread.run_sync(_run)
