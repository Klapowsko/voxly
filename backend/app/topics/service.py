from pathlib import Path
import re

from anyio import to_thread

from app.config import Settings


PROMPT_TOPICOS = """Voc√™ √© um especialista em an√°lise e organiza√ß√£o de conte√∫do. Analise o seguinte texto transcrito de um √°udio e crie uma estrutura de t√≥picos PROFUNDA, DETALHADA e COMPLETA, cobrindo TODOS os assuntos importantes mencionados.

INSTRU√á√ïES DETALHADAS:

1. COMPREENS√ÉO COMPLETA DO CONTE√öDO:
   - Leia e compreenda COMPLETAMENTE todo o texto, sem pular partes
   - Identifique TODOS os principais temas, conceitos e ideias apresentados
   - Entenda o contexto e a mensagem central
   - Reconhe√ßa refer√™ncias b√≠blicas, hist√≥ricas ou culturais mencionadas
   - Identifique TODAS as transi√ß√µes de assunto e mudan√ßas de tema
   - N√£o deixe nenhum assunto importante de fora

2. ORGANIZA√á√ÉO EM T√ìPICOS (GERE T√ìPICOS PARA TODOS OS ASSUNTOS):
   - Crie t√≥picos tem√°ticos para CADA assunto importante mencionado
   - N√ÉO limite a quantidade de t√≥picos - crie quantos forem necess√°rios para cobrir todo o conte√∫do
   - Cada t√≥pico deve ter um T√çTULO DESCRITIVO e espec√≠fico que resume o conte√∫do
   - N√ÉO agrupe assuntos diferentes em um √∫nico t√≥pico - cada assunto importante deve ter seu pr√≥prio t√≥pico
   - Agrupe apenas ideias MUITO relacionadas no mesmo t√≥pico
   - Use subt√≥picos quando necess√°rio para melhor organiza√ß√£o
   - Identifique CADA mudan√ßa de assunto como um novo t√≥pico
   - Seja generoso na cria√ß√£o de t√≥picos - √© melhor ter mais t√≥picos bem organizados do que poucos t√≥picos gen√©ricos

3. CONTE√öDO DETALHADO E COMPLETO (N√ÉO SEJA SUCINTO):
   - Cada t√≥pico deve ter CONTE√öDO SUBSTANCIAL e completo
   - Inclua TODO o contexto relevante do que foi dito sobre aquele assunto
   - Adicione explica√ß√µes e detalhes importantes
   - N√ÉO resuma demais - preserve TODAS as informa√ß√µes relevantes
   - Inclua cita√ß√µes diretas quando forem importantes
   - Desenvolva cada ideia com profundidade e completude
   - Certifique-se de que nenhuma informa√ß√£o importante seja perdida

4. COMENT√ÅRIOS E AN√ÅLISES DA IA:
   - Adicione uma se√ß√£o de "üí° An√°lise" ou "üìù Coment√°rios" no in√≠cio de cada t√≥pico
   - Forne√ßa insights, interpreta√ß√µes e observa√ß√µes sobre o conte√∫do
   - Destaque pontos-chave importantes e sua relev√¢ncia
   - Adicione conex√µes entre ideias quando apropriado
   - Forne√ßa contexto adicional quando necess√°rio
   - Se houver refer√™ncias b√≠blicas, explique seu significado e contexto
   - Adicione observa√ß√µes sobre a import√¢ncia ou aplica√ß√£o pr√°tica do conte√∫do

5. FORMATA√á√ÉO:
   - Use par√°grafos bem formatados (n√£o tudo em uma linha)
   - Quebre o texto em par√°grafos l√≥gicos de 3-5 linhas
   - Use formata√ß√£o Markdown apropriada (## para t√≠tulos, ** para √™nfase, - para listas)
   - Adicione espa√ßamento adequado entre se√ß√µes
   - Use emojis para destacar se√ß√µes importantes (üí°, üìù, ‚ö†Ô∏è, etc.)

6. ESTRUTURA ESPERADA PARA CADA T√ìPICO:
   ```
   ## [T√≠tulo Descritivo e Espec√≠fico do T√≥pico]
   
   üí° **An√°lise e Coment√°rios:**
   [Sua an√°lise completa, insights e coment√°rios sobre este t√≥pico - explique o que √© importante, por que √© relevante, e adicione contexto detalhado]
   
   **Conte√∫do:**
   [Conte√∫do completo e detalhado do t√≥pico, com m√∫ltiplos par√°grafos desenvolvendo TODAS as ideias relacionadas]
   
   [Mais par√°grafos com detalhes adicionais e informa√ß√µes complementares]
   
   [Subt√≥picos se necess√°rio para organizar melhor o conte√∫do]
   ```

7. COBERTURA COMPLETA:
   - Analise TODO o texto, do in√≠cio ao fim
   - Crie t√≥picos para TODOS os assuntos importantes mencionados
   - N√£o deixe nenhuma parte significativa do conte√∫do sem um t√≥pico correspondente
   - Adapte a quantidade de t√≥picos ao tamanho e complexidade do conte√∫do
   - Para conte√∫dos longos (1 hora ou mais), crie t√≥picos suficientes para cobrir tudo
   - Para conte√∫dos mais curtos, ainda assim crie t√≥picos detalhados para cada assunto

TEXTO A ANALISAR:
{texto}

Gere os t√≥picos organizados seguindo EXATAMENTE as instru√ß√µes acima. Seja MUITO detalhado, formatado, inteligente na organiza√ß√£o e gere t√≥picos COMPLETOS para TODOS os assuntos importantes, sem deixar nada de fora. O objetivo √© ter uma cobertura completa e detalhada de todo o conte√∫do."""


def formatar_resultado_ia(texto: str) -> str:
    """Formata o resultado da IA para garantir qualidade."""
    # Remove espa√ßos m√∫ltiplos
    texto = re.sub(r" +", " ", texto)
    # Garante quebra de linha ap√≥s t√≠tulos
    texto = re.sub(r"(## .+?)([^\n])", r"\1\n\2", texto)
    # Garante par√°grafos (quebra dupla ap√≥s par√°grafos longos)
    texto = re.sub(r"\. ([A-Z][^.!?]{50,})", r".\n\n\1", texto)
    return texto.strip()


def usar_ollama(texto: str, modelo: str = "llama3.2", ollama_url: str = "http://localhost:11434", request_id: str | None = None) -> str | None:
    """Usa Ollama para gerar t√≥picos (requer servidor Ollama rodando)."""
    try:
        import ollama
    except ImportError:
        print(f"[{request_id or 'N/A'}] Ollama n√£o dispon√≠vel (biblioteca n√£o instalada)")
        return None
    
    # Usa o texto completo ou o m√°ximo que o modelo suportar
    # Para modelos grandes, podemos usar textos muito longos
    # Limite aumentado significativamente para suportar √°udios longos
    texto_limitado = texto[:100000] if len(texto) > 100000 else texto
    if len(texto) > 100000:
        print(f"[{request_id or 'N/A'}] Texto limitado para {len(texto_limitado)} caracteres (original: {len(texto)})")
    
    prompt = PROMPT_TOPICOS.format(texto=texto_limitado)
    print(f"[{request_id or 'N/A'}] Enviando prompt para Ollama (tamanho: {len(prompt)} caracteres)...")
    
    try:
        # Tenta usar a biblioteca ollama
        print(f"[{request_id or 'N/A'}] Chamando Ollama.chat() com modelo {modelo}...")
        response = ollama.chat(
            model=modelo,
            messages=[
                {
                    "role": "system",
                    "content": "Voc√™ √© um especialista em an√°lise de conte√∫do, organiza√ß√£o de informa√ß√µes e formata√ß√£o de textos. Sempre siga as instru√ß√µes detalhadamente. Gere M√öLTIPLOS t√≥picos com conte√∫do substancial e coment√°rios anal√≠ticos."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            options={
                "temperature": 0.4,  # Um pouco mais de criatividade para coment√°rios
                "num_predict": 16000,  # Permite respostas muito longas para conte√∫dos extensos
            }
        )
        resultado = response["message"]["content"]
        print(f"[{request_id or 'N/A'}] Resposta recebida do Ollama: {len(resultado)} caracteres")
        return formatar_resultado_ia(resultado)
    except Exception as e:
        print(f"[{request_id or 'N/A'}] Erro ao usar biblioteca Ollama: {e}")
        # Se falhar, tenta usar API HTTP diretamente
        try:
            print(f"[{request_id or 'N/A'}] Tentando usar API HTTP do Ollama...")
            import requests
            response = requests.post(
                f"{ollama_url}/api/chat",
                json={
                    "model": modelo,
                    "messages": [
                        {
                            "role": "system",
                            "content": "Voc√™ √© um especialista em an√°lise de conte√∫do, organiza√ß√£o de informa√ß√µes e formata√ß√£o de textos. Sempre siga as instru√ß√µes detalhadamente."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "stream": False,
                    "options": {
                        "temperature": 0.4,
                        "num_predict": 16000,
                    }
                },
                timeout=600  # 10 minutos
            )
            if response.status_code == 200:
                resultado = response.json()["message"]["content"]
                print(f"[{request_id or 'N/A'}] Resposta recebida via HTTP: {len(resultado)} caracteres")
                return formatar_resultado_ia(resultado)
            else:
                print(f"[{request_id or 'N/A'}] Erro HTTP do Ollama: {response.status_code}")
        except Exception as e:
            print(f"[{request_id or 'N/A'}] Erro ao usar API HTTP do Ollama: {e}")
            return None
    
    return None


def usar_huggingface(texto: str, request_id: str | None = None) -> str | None:
    """Usa Hugging Face Transformers para an√°lise e organiza√ß√£o."""
    try:
        from transformers import pipeline
    except ImportError:
        print(f"[{request_id or 'N/A'}] Hugging Face n√£o dispon√≠vel (biblioteca n√£o instalada)")
        return None
    
    try:
        print(f"[{request_id or 'N/A'}] Carregando modelo Hugging Face (pode demorar na primeira vez)...")
        # Usa um modelo de sumariza√ß√£o para extrair pontos-chave
        summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=-1  # CPU
        )
        print(f"[{request_id or 'N/A'}] Modelo Hugging Face carregado")
        
        # Divide o texto em chunks menores para processar mais conte√∫do
        # Chunks menores permitem mais t√≥picos
        palavras = texto.split()
        palavras_por_chunk = 400  # Chunks menores para mais granularidade
        num_chunks = max(10, len(palavras) // palavras_por_chunk)  # M√≠nimo 10 chunks, mais se necess√°rio
        print(f"[{request_id or 'N/A'}] Dividindo texto em {num_chunks} chunks...")
        
        chunks = []
        for i in range(0, len(palavras), palavras_por_chunk):
            chunk = " ".join(palavras[i:i + palavras_por_chunk])
            if len(chunk.strip()) > 50:  # Apenas chunks significativos
                chunks.append(chunk)
        
        # Processa cada chunk e preserva conte√∫do original
        topicos_com_conteudo = []
        
        for i, chunk in enumerate(chunks):
            try:
                if request_id and (i + 1) % 3 == 0:  # Atualiza a cada 3 chunks
                    progresso = 75 + int((i + 1) / len(chunks) * 15)
                    from app.utils.status import set_status
                    set_status(request_id, "generating", progresso, f"Processando chunk {i + 1}/{len(chunks)}...")
                
                # Gera resumo do chunk
                resultado = summarizer(
                    chunk,
                    max_length=200,  # Resumos maiores
                    min_length=80,   # M√≠nimo maior para mais detalhes
                    do_sample=False
                )
                resumo = resultado[0]["summary_text"]
                
                # Identifica tema automaticamente do chunk
                tema = identificar_tema_automatico(chunk, resumo)
                
                # Preserva parte do conte√∫do original junto com o resumo
                topicos_com_conteudo.append({
                    "tema": tema,
                    "resumo": resumo,
                    "conteudo_original": chunk[:500],  # Primeiros 500 caracteres do conte√∫do original
                    "indice": i
                })
            except Exception as e:
                print(f"[{request_id or 'N/A'}] Erro ao processar chunk {i + 1}: {e}")
                continue
        
        if topicos_com_conteudo:
            return formatar_topicos_huggingface_melhorado(topicos_com_conteudo)
        
        return None
    except Exception:
        return None


def identificar_tema_automatico(conteudo_original: str, resumo: str) -> str:
    """Identifica o tema principal automaticamente baseado no conte√∫do, sem usar lista fixa."""
    from collections import Counter
    
    # Combina conte√∫do e resumo para an√°lise
    texto_completo = (conteudo_original + " " + resumo).strip()
    
    # Extrai palavras-chave importantes (palavras com 4+ letras)
    palavras = re.findall(r"\b\w{4,}\b", texto_completo.lower())
    
    # Remove palavras comuns que n√£o s√£o informativas
    palavras_comuns = {
        "isso", "aqui", "onde", "quando", "como", "para", "com", "sobre",
        "mais", "muito", "pode", "ser√°", "seria", "tempo", "momento", "pessoa",
        "pessoas", "coisa", "coisas", "tipo", "tipos", "forma", "maneira",
        "tamb√©m", "ainda", "sempre", "nunca", "depois", "antes", "agora",
        "ent√£o", "assim", "dessa", "desse", "deste", "desta", "todo", "toda"
    }
    
    palavras_filtradas = [p for p in palavras if p not in palavras_comuns]
    
    # Conta frequ√™ncia das palavras
    contador = Counter(palavras_filtradas)
    
    # Pega as palavras mais frequentes e significativas
    palavras_principais = [palavra for palavra, _ in contador.most_common(8)]
    
    # Tenta criar t√≠tulo inteligente baseado no conte√∫do
    # Primeiro, tenta usar o in√≠cio do resumo se for descritivo
    if resumo:
        primeira_frase = resumo.split(".")[0].strip()
        # Se a primeira frase for razo√°vel (20-100 caracteres), usa ela
        if 20 <= len(primeira_frase) <= 100:
            # Limpa a frase removendo palavras muito comuns no in√≠cio
            palavras_frase = primeira_frase.split()
            if len(palavras_frase) > 3:
                return primeira_frase
    
    # Se n√£o, cria t√≠tulo baseado nas palavras principais
    if palavras_principais:
        # Remove duplicatas e palavras muito similares
        palavras_unicas = []
        palavras_vistas = set()
        for palavra in palavras_principais:
            # Normaliza a palavra (remove varia√ß√µes)
            palavra_normalizada = palavra.lower().strip()
            # Evita palavras muito similares (ex: "amigo" e "amigos")
            if palavra_normalizada not in palavras_vistas:
                # Verifica se n√£o √© varia√ß√£o de palavra j√° vista
                is_variacao = any(
                    palavra_normalizada.startswith(p[:4]) or p.startswith(palavra_normalizada[:4])
                    for p in palavras_vistas
                )
                if not is_variacao:
                    palavras_unicas.append(palavra)
                    palavras_vistas.add(palavra_normalizada)
        
        # Pega as 2-3 palavras mais relevantes (√∫nicas)
        palavras_titulo = palavras_unicas[:3]
        if palavras_titulo:
            titulo = " ".join(palavras_titulo).title()
            # Limpa espa√ßos m√∫ltiplos
            titulo = re.sub(r"\s+", " ", titulo).strip()
            
            # Se o t√≠tulo for muito curto, adiciona contexto
            if len(titulo) < 15:
                # Tenta pegar uma frase do conte√∫do original
                primeira_sentenca = conteudo_original.split(".")[0].strip()
                if 30 <= len(primeira_sentenca) <= 80:
                    return primeira_sentenca[:60]
            
            return titulo
    
    # Fallback: usa in√≠cio do conte√∫do
    primeira_parte = conteudo_original[:70].strip()
    if primeira_parte:
        # Remove pontua√ß√£o final se houver
        primeira_parte = re.sub(r"[.!?]+$", "", primeira_parte)
        return primeira_parte
    
    return "T√≥pico do Conte√∫do"


def formatar_topicos_huggingface_melhorado(topicos_com_conteudo: list) -> str:
    """Formata os t√≥picos do Hugging Face com conte√∫do detalhado."""
    resultado = "# T√≥picos Organizados da Transcri√ß√£o\n\n"
    resultado += "*An√°lise realizada com Hugging Face Transformers*\n\n"
    
    # Agrupa t√≥picos por tema, mas mant√©m separados se forem temas diferentes
    topicos_agrupados = {}
    for topico in topicos_com_conteudo:
        tema = topico["tema"]
        if tema not in topicos_agrupados:
            topicos_agrupados[tema] = []
        topicos_agrupados[tema].append(topico)
    
    # Cria um t√≥pico para cada grupo, mas se houver muitos do mesmo tema, divide
    indice = 1
    for tema, topicos_tema in topicos_agrupados.items():
        # Se houver muitos t√≥picos do mesmo tema, divide em subt√≥picos
        if len(topicos_tema) > 2:
            # Cria um t√≥pico principal e subt√≥picos
            resultado += f"## {indice}. {tema}\n\n"
            resultado += "üí° **An√°lise:**\n"
            resultado += f"Este tema √© abordado em {len(topicos_tema)} momentos diferentes do conte√∫do, demonstrando sua import√¢ncia.\n\n"
            resultado += "**Conte√∫do:**\n\n"
            
            for sub_indice, topico in enumerate(topicos_tema, 1):
                resultado += f"### {indice}.{sub_indice} - {tema} (Parte {sub_indice})\n\n"
                resultado += f"**Resumo:** {topico['resumo']}\n\n"
                resultado += f"**Conte√∫do detalhado:**\n\n"
                # Formata o conte√∫do original em par√°grafos
                conteudo = topico["conteudo_original"]
                sentencas = re.split(r"([.!?]+)", conteudo)
                paragrafo = []
                
                for j in range(0, len(sentencas) - 1, 2):
                    if j + 1 < len(sentencas):
                        sentenca = (sentencas[j] + sentencas[j + 1]).strip()
                        if sentenca and len(sentenca) > 20:
                            paragrafo.append(sentenca)
                            if len(paragrafo) >= 2:
                                resultado += " ".join(paragrafo) + "\n\n"
                                paragrafo = []
                
                if paragrafo:
                    resultado += " ".join(paragrafo) + "\n\n"
                
                resultado += "\n"
            
            resultado += "---\n\n"
            indice += 1
        else:
            # T√≥picos √∫nicos ou poucos - cria t√≥pico individual
            for topico in topicos_tema:
                resultado += f"## {indice}. {tema}\n\n"
                resultado += "üí° **An√°lise:**\n"
                resultado += f"{topico['resumo']}\n\n"
                resultado += "**Conte√∫do detalhado:**\n\n"
                
                # Formata o conte√∫do original em par√°grafos
                conteudo = topico["conteudo_original"]
                sentencas = re.split(r"([.!?]+)", conteudo)
                paragrafo = []
                
                for j in range(0, len(sentencas) - 1, 2):
                    if j + 1 < len(sentencas):
                        sentenca = (sentencas[j] + sentencas[j + 1]).strip()
                        if sentenca and len(sentenca) > 20:
                            paragrafo.append(sentenca)
                            if len(paragrafo) >= 2:
                                resultado += " ".join(paragrafo) + "\n\n"
                                paragrafo = []
                
                if paragrafo:
                    resultado += " ".join(paragrafo) + "\n\n"
                
                resultado += "---\n\n"
                indice += 1
    
    return resultado


def gerar_topicos_simples(texto: str) -> str:
    """Gera t√≥picos melhorados usando processamento de texto inteligente (fallback final)."""
    # Remove espa√ßos m√∫ltiplos
    texto = re.sub(r"\s+", " ", texto).strip()
    
    # Calcula quantos t√≥picos criar baseado no tamanho do texto
    palavras = texto.split()
    num_palavras = len(palavras)
    # Adapta dinamicamente: aproximadamente 1 t√≥pico a cada 200-300 palavras
    # Mas permite mais t√≥picos para conte√∫dos maiores
    palavras_por_topico = max(200, min(300, num_palavras // max(6, num_palavras // 500)))
    
    # Divide o texto em partes baseado em pontua√ß√£o e tamanho
    partes = re.split(r"([.!?]+)", texto)
    
    # Reconstr√≥i senten√ßas completas
    sentencas = []
    buffer = ""
    for parte in partes:
        parte = parte.strip()
        if not parte:
            continue
        
        # Se a parte tem pontua√ß√£o ou √© muito longa, finaliza senten√ßa
        if re.search(r"[.!?]$", parte) or len(buffer) > 150:
            if buffer:
                sentenca_completa = (buffer + " " + parte).strip()
                if len(sentenca_completa) > 40:  # Apenas senten√ßas significativas
                    sentencas.append(sentenca_completa)
                buffer = ""
            else:
                if len(parte) > 40:
                    sentencas.append(parte)
        else:
            buffer = (buffer + " " + parte).strip()
    
    if buffer and len(buffer) > 40:
        sentencas.append(buffer)
    
    # Se ainda n√£o tiver senten√ßas suficientes, divide por tamanho
    if len(sentencas) < palavras_por_topico:
        palavras = texto.split()
        tamanho_chunk = palavras_por_topico
        sentencas = []
        for i in range(0, len(palavras), tamanho_chunk):
            chunk = " ".join(palavras[i:i + tamanho_chunk])
            if len(chunk) > 40:
                sentencas.append(chunk)
    
    # Divide senten√ßas em grupos para criar t√≥picos dinamicamente
    # Cria m√∫ltiplos t√≥picos baseado no conte√∫do - sem limite m√°ximo r√≠gido
    # Adapta ao tamanho: mais senten√ßas = mais t√≥picos poss√≠veis
    num_topicos = max(6, len(sentencas) // 3)  # M√≠nimo 6, mas pode ser muito mais para conte√∫dos longos
    sentencas_por_topico = len(sentencas) // num_topicos if num_topicos > 0 else len(sentencas)
    
    # Cria t√≥picos dinamicamente com identifica√ß√£o autom√°tica de temas
    from collections import Counter
    
    resultado = "# T√≥picos Organizados da Transcri√ß√£o\n\n"
    resultado += "*An√°lise e organiza√ß√£o inteligente do conte√∫do*\n\n"
    
    for i in range(num_topicos):
        inicio = i * sentencas_por_topico
        fim = inicio + sentencas_por_topico if i < num_topicos - 1 else len(sentencas)
        sentencas_topo = sentencas[inicio:fim]
        
        if not sentencas_topo:
            continue
        
        # Identifica tema automaticamente baseado no conte√∫do deste t√≥pico
        conteudo_topo = " ".join(sentencas_topo)
        
        # Extrai palavras-chave do t√≥pico
        palavras_topo = re.findall(r"\b\w{4,}\b", conteudo_topo.lower())
        palavras_comuns = {
            "isso", "aqui", "onde", "quando", "como", "para", "com", "sobre",
            "mais", "muito", "pode", "ser√°", "seria", "tempo", "momento"
        }
        palavras_filtradas = [p for p in palavras_topo if p not in palavras_comuns]
        
        # Conta frequ√™ncia
        contador = Counter(palavras_filtradas)
        palavras_principais = [palavra for palavra, _ in contador.most_common(5)]
        
        # Cria t√≠tulo baseado nas palavras principais ou primeira senten√ßa
        if palavras_principais:
            # Remove duplicatas e palavras muito similares
            palavras_unicas = []
            palavras_vistas = set()
            for palavra in palavras_principais:
                palavra_normalizada = palavra.lower().strip()
                # Evita palavras muito similares (ex: "amigo" e "amigos")
                if palavra_normalizada not in palavras_vistas:
                    is_variacao = any(
                        palavra_normalizada.startswith(p[:4]) or p.startswith(palavra_normalizada[:4])
                        for p in palavras_vistas
                    )
                    if not is_variacao:
                        palavras_unicas.append(palavra)
                        palavras_vistas.add(palavra_normalizada)
            
            if palavras_unicas:
                titulo = " ".join(palavras_unicas[:3]).title()
                # Limpa espa√ßos m√∫ltiplos e caracteres estranhos
                titulo = re.sub(r"\s+", " ", titulo).strip()
                
                if len(titulo) < 10:
                    # Se t√≠tulo muito curto, usa primeira senten√ßa
                    primeira_sentenca = sentencas_topo[0].strip()
                    if len(primeira_sentenca) > 20:
                        titulo = primeira_sentenca[:60].rstrip(".,!?")
                        titulo = re.sub(r"\s+", " ", titulo).strip()
            else:
                # Fallback se n√£o houver palavras √∫nicas
                primeira_sentenca = sentencas_topo[0].strip()
                titulo = primeira_sentenca[:60].rstrip(".,!?") if len(primeira_sentenca) > 20 else f"T√≥pico {i + 1}"
        else:
            # Usa primeira senten√ßa como t√≠tulo
            primeira_sentenca = sentencas_topo[0].strip()
            titulo = primeira_sentenca[:60].rstrip(".,!?") if len(primeira_sentenca) > 20 else f"T√≥pico {i + 1}"
            titulo = re.sub(r"\s+", " ", titulo).strip()
        
        resultado += f"## {i + 1}. {titulo}\n\n"
        resultado += "üí° **An√°lise:**\n"
        # Cria an√°lise mais inteligente baseada no conte√∫do
        if palavras_principais and len(palavras_principais) > 0:
            # Remove duplicatas para a an√°lise tamb√©m
            palavras_analise = list(dict.fromkeys(palavras_principais[:3]))  # Mant√©m ordem, remove duplicatas
            if palavras_analise:
                resultado += f"Este t√≥pico aborda aspectos relacionados a {', '.join(palavras_analise)}. "
            else:
                resultado += "Este t√≥pico aborda aspectos importantes do conte√∫do apresentado. "
        else:
            resultado += "Este t√≥pico aborda aspectos importantes do conte√∫do apresentado. "
        resultado += "O conte√∫do desenvolve ideias importantes sobre este tema.\n\n"
        resultado += "**Conte√∫do:**\n\n"
        
        # Formata senten√ßas em par√°grafos bem estruturados
        paragrafo = []
        for sentenca in sentencas_topo:
            # Adiciona pontua√ß√£o se n√£o tiver
            if not re.search(r"[.!?]$", sentenca):
                sentenca += "."
            
            paragrafo.append(sentenca)
            
            # Cria par√°grafos de 2-3 senten√ßas
            if len(paragrafo) >= 2:
                texto_paragrafo = " ".join(paragrafo)
                resultado += texto_paragrafo + "\n\n"
                paragrafo = []
        
        # Adiciona par√°grafo restante
        if paragrafo:
            texto_paragrafo = " ".join(paragrafo)
            if not re.search(r"[.!?]$", texto_paragrafo):
                texto_paragrafo += "."
            resultado += texto_paragrafo + "\n\n"
        
        resultado += "---\n\n"
    
    return resultado


async def generate_topics_markdown(
    transcript: str, settings: Settings, request_id: str, request_id_status: str | None = None
) -> tuple[str, Path]:
    """Gera conte√∫do em Markdown usando modelos open source (Ollama/Hugging Face/m√©todo simples)."""
    output_path = settings.outputs_dir / f"{request_id}_topics.md"
    
    def _run() -> tuple[str, Path]:
        import time
        from app.utils.status import set_status
        
        resultado = None
        start_time = time.time()
        tamanho_texto = len(transcript)
        num_palavras = len(transcript.split())
        
        print(f"[{request_id_status or request_id}] Iniciando gera√ß√£o de t√≥picos...")
        print(f"[{request_id_status or request_id}] Tamanho do texto: {tamanho_texto} caracteres, ~{num_palavras} palavras")
        
        if request_id_status:
            set_status(request_id_status, "generating", 70, f"Analisando texto ({num_palavras} palavras)...")
        
        # Tenta Ollama primeiro
        if settings.ollama_model:
            print(f"[{request_id_status or request_id}] Tentando usar Ollama (modelo: {settings.ollama_model})...")
            if request_id_status:
                set_status(request_id_status, "generating", 75, f"Gerando t√≥picos com Ollama ({settings.ollama_model})...")
            
            resultado = usar_ollama(transcript, settings.ollama_model, settings.ollama_url, request_id_status)
            if resultado:
                print(f"[{request_id_status or request_id}] ‚úì T√≥picos gerados com Ollama ({len(resultado)} caracteres)")
                if request_id_status:
                    set_status(request_id_status, "generating", 90, f"T√≥picos gerados com Ollama ({len(resultado)} caracteres)")
                output_path.write_text(resultado, encoding="utf-8")
                return resultado, output_path
            else:
                print(f"[{request_id_status or request_id}] Ollama n√£o dispon√≠vel ou falhou")
        
        # Se Ollama n√£o funcionou, tenta Hugging Face
        if not resultado:
            print(f"[{request_id_status or request_id}] Tentando usar Hugging Face...")
            if request_id_status:
                set_status(request_id_status, "generating", 75, "Gerando t√≥picos com Hugging Face...")
            
            resultado = usar_huggingface(transcript, request_id_status)
            if resultado:
                print(f"[{request_id_status or request_id}] ‚úì T√≥picos gerados com Hugging Face ({len(resultado)} caracteres)")
                if request_id_status:
                    set_status(request_id_status, "generating", 90, f"T√≥picos gerados com Hugging Face ({len(resultado)} caracteres)")
                output_path.write_text(resultado, encoding="utf-8")
                return resultado, output_path
            else:
                print(f"[{request_id_status or request_id}] Hugging Face n√£o dispon√≠vel ou falhou")
        
        # Fallback para m√©todo simples
        if not resultado or len(resultado.strip()) < 100:
            print(f"[{request_id_status or request_id}] Usando m√©todo simples (fallback)...")
            if request_id_status:
                set_status(request_id_status, "generating", 80, "Gerando t√≥picos com m√©todo simples...")
            
            resultado = gerar_topicos_simples(transcript)
            print(f"[{request_id_status or request_id}] ‚úì T√≥picos gerados com m√©todo simples ({len(resultado)} caracteres)")
        
        # Garante que o resultado n√£o est√° vazio
        if not resultado or len(resultado.strip()) < 50:
            print(f"[{request_id_status or request_id}] Resultado muito curto, regenerando...")
            resultado = gerar_topicos_simples(transcript)
        
        elapsed = time.time() - start_time
        print(f"[{request_id_status or request_id}] Gera√ß√£o de t√≥picos conclu√≠da em {elapsed:.1f}s")
        print(f"[{request_id_status or request_id}] Resultado final: {len(resultado)} caracteres, ~{len(resultado.split())} palavras")
        
        if request_id_status:
            num_topicos = resultado.count("##")
            set_status(request_id_status, "generating", 92, f"T√≥picos formatados: {num_topicos} t√≥picos identificados")
        
        output_path.write_text(resultado, encoding="utf-8")
        return resultado, output_path
    
    return await to_thread.run_sync(_run)
